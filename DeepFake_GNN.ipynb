{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otSMnw4HSURU",
        "outputId": "12c7cac4-b1a8-4015-e3c9-5849ad71a9ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: dlib in /usr/local/lib/python3.11/dist-packages (19.24.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision opencv-python dlib matplotlib numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import dlib\n",
        "\n",
        "# Load pre-trained face detector and landmark predictor\n",
        "detector = dlib.get_frontal_face_detector()\n",
        "# Replace with your data\n",
        "predictor = dlib.shape_predictor(\"/content/drive/MyDrive/Datasets/shape_predictor_68_face_landmarks.dat\")  # Ensure this file is downloaded\n",
        "\n",
        "def detect_and_align_face(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    faces = detector(gray)\n",
        "\n",
        "    for face in faces:\n",
        "        landmarks = predictor(gray, face)\n",
        "\n",
        "        # Extract the coordinates of the face landmarks\n",
        "        landmarks = [(landmarks.part(i).x, landmarks.part(i).y) for i in range(68)]\n",
        "\n",
        "        # Align the face (optional: you can use affine transformation for better alignment)\n",
        "        aligned_face = image[face.top():face.bottom(), face.left():face.right()]\n",
        "        return aligned_face\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "xYyhuv2ocrv4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "\n",
        "def generate_deepfake(video_path, output_path):\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video file.\")\n",
        "        return\n",
        "\n",
        "    # Get the total number of frames in the video\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Set up the video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Use a widely supported codec\n",
        "    out = cv2.VideoWriter(output_path, fourcc, 30.0, (128, 128))  # Ensure frame size matches the model's output\n",
        "\n",
        "    # Initialize tqdm progress bar\n",
        "    progress_bar = tqdm(total=total_frames, desc=\"Processing Frames\", unit=\"frame\")\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Detect and align the face\n",
        "            face = detect_and_align_face(frame)\n",
        "\n",
        "            if face is not None:\n",
        "                # Resize the face to match the model's input size\n",
        "                face = cv2.resize(face, (128, 128))\n",
        "\n",
        "                # Normalize the face image\n",
        "                face = face / 255.0\n",
        "\n",
        "                # Convert the face to a tensor and pass it through the model\n",
        "                face_tensor = torch.from_numpy(face).permute(2, 0, 1).unsqueeze(0).float().to(device)\n",
        "                with torch.no_grad():\n",
        "                    fake_face = model(face_tensor).squeeze(0).cpu().numpy()\n",
        "                    fake_face = np.transpose(fake_face, (1, 2, 0)) * 255\n",
        "                    fake_face = fake_face.astype(np.uint8)\n",
        "\n",
        "                # Write the fake face to the output video\n",
        "                out.write(fake_face)\n",
        "            else:\n",
        "                # Write the original frame (or a placeholder) if no face is detected\n",
        "                out.write(cv2.resize(frame, (128, 128)))\n",
        "\n",
        "            # Update the progress bar\n",
        "            progress_bar.update(1)\n",
        "    finally:\n",
        "        # Release resources\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        progress_bar.close()  # Close the progress bar\n",
        "\n",
        "    print(\"\\nDeepFake video generated successfully!\")"
      ],
      "metadata": {
        "id": "MZAM8_RSc0MQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the DeepFake video\n",
        "generate_deepfake(\"/content/6260385433993.mp4\", \"deepfake_output.mp4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_JXgLivdAxY",
        "outputId": "c5a6a595-4480-4f0f-b8ba-bc83572c6f3e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Frames: 100%|██████████| 309/309 [00:21<00:00, 14.54frame/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DeepFake video generated successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}